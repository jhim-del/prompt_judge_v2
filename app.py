import streamlit as st
import pandas as pd
import json
import os
import asyncio
import time
import io
import re
from openai import AsyncOpenAI, RateLimitError
from pypdf import PdfReader

def extract_csv_dataframes(text):
    """
    LLM ë‹µë³€ í…ìŠ¤íŠ¸ì—ì„œ CSV ë¸”ë¡(ë“¤)ì„ ì°¾ì•„ DataFrame ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    """
    dfs = []
    
    # 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡(```csv ... ```) ì¶”ì¶œ ì‹œë„
    code_blocks = re.findall(r'```csv\s*([\s\S]*?)\s*```', text, re.IGNORECASE)
    
    if not code_blocks:
        # csv íƒœê·¸ê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ì½”ë“œ ë¸”ë¡(``` ... ```) ì‹œë„
        code_blocks = re.findall(r'```\s*([\s\S]*?)\s*```', text)
    
    # ë¸”ë¡ì´ ë°œê²¬ë˜ë©´ ê°ê° íŒŒì‹±
    if code_blocks:
        for block in code_blocks:
            try:
                dfs.append(pd.read_csv(io.StringIO(block.strip())))
            except:
                continue
    else:
        # ë¸”ë¡ì´ ì•„ì˜ˆ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ CSVë¡œ ì‹œë„
        try:
            dfs.append(pd.read_csv(io.StringIO(text.strip())))
        except:
            pass
            
    return dfs

# ---------------------------------------------------------
# [ì„¤ì •] í˜ì´ì§€ ê¸°ë³¸ ì„¸íŒ…
# ---------------------------------------------------------
st.set_page_config(page_title="DB Inc í”„ë¡¬í”„íŠ¸ ê²½ì§„ëŒ€íšŒ ì±„ì ê¸° v2.1", layout="wide", page_icon="âš–ï¸")

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
    <style>
    .metric-container { background-color: #f8f9fa; padding: 15px; border-radius: 10px; border: 1px solid #dee2e6; }
    .status-box { 
        padding: 15px; border-radius: 8px; margin-bottom: 10px; text-align: center; 
        font-size: 1.1rem; background-color: #e3f2fd; border: 1px solid #90caf9; 
        color: #1565c0; font-weight: bold;
    }
    .success-box { background-color: #e8f5e9; color: #2e7d32; border-color: #c8e6c9; }
    </style>
""", unsafe_allow_html=True)

# ---------------------------------------------------------
# [í•¨ìˆ˜] íŒŒì¼ ì²˜ë¦¬ ë° ìœ í‹¸ë¦¬í‹°
# ---------------------------------------------------------
def read_file_content(file):
    """íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if not file: return None
    ext = file.name.split('.')[-1].lower()
    try:
        if ext == 'pdf':
            reader = PdfReader(file)
            return "".join([page.extract_text() for page in reader.pages])
        elif ext in ['xlsx', 'xls']:
            # ì—‘ì…€ì€ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì„œ ë¬¸ë§¥ìœ¼ë¡œ ì œê³µ
            sheets = pd.read_excel(file, sheet_name=None)
            text = []
            for name, df in sheets.items():
                text.append(f"### Sheet: {name}\n{df.to_markdown(index=False)}")
            return "\n\n".join(text)
        elif ext == 'csv':
            return pd.read_csv(file).to_markdown(index=False)
        else: # txt, md, py etc
            return file.getvalue().decode("utf-8")
    except Exception as e:
        return f"Error reading file: {str(e)}"

def load_golden_excel(file):
    """ê³¼ì œ B ì±„ì ìš© ì •ë‹µ ì—‘ì…€ ë¡œë“œ"""
    if file and file.name.endswith('.xlsx'):
        return pd.read_excel(file, sheet_name=None)
    return None

# ---------------------------------------------------------
# [í•µì‹¬] ë¹„ë™ê¸° LLM í†µì‹  ë° ì‹¤í–‰
# ---------------------------------------------------------
async def safe_api_call(client, model, messages, temperature=0, response_format=None):
    try:
        return await client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            response_format=response_format
        )
    except Exception as e:
        return None

async def execute_participant_prompt(client, model, context, prompt, task_type):
    """ì°¸ê°€ì í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ (Executor)"""
    
    system_instruction = "ë‹¹ì‹ ì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§€ì‹œë¥¼ ì •í™•íˆ ë”°ë¥´ì„¸ìš”."
    
    # ê³¼ì œ Bì˜ ê²½ìš°, íŒŒì‹± ê°€ëŠ¥í•œ í¬ë§·ì„ ê°•ì œí•˜ê¸° ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì£¼ì…
    if task_type == "Task B (ë°ì´í„° ì •ì œ)":
        system_instruction += "\n[ì¤‘ìš”] ê²°ê³¼ë¬¼ì€ ë°˜ë“œì‹œ CSV í¬ë§·(ì½¤ë§ˆ êµ¬ë¶„)ìœ¼ë¡œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ë§ì€ í•˜ì§€ ë§ˆì„¸ìš”."

    messages = [
        {"role": "system", "content": system_instruction},
        {"role": "user", "content": f"---[Context Data]---\n{context}\n\n---[Instruction]---\n{prompt}"}
    ]
    
    resp = await safe_api_call(client, model, messages, temperature=0)
    return resp.choices[0].message.content if resp else "Error"

# ---------------------------------------------------------
# [í‰ê°€ ë¡œì§ 1] ê³¼ì œ A/C : LLM Judge (Atomic Checklist)
# ---------------------------------------------------------
async def evaluate_text_logic(client, model, target_text, user_output, task_type):
    """LLMì„ ì´ìš©í•œ ë…¼ë¦¬/êµ¬ì¡° í‰ê°€"""
    
    # ê³¼ì œë³„ ì²´í¬ë¦¬ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë¶„ê¸°
    if "Task A" in task_type:
        checklist_prompt = """
        1. [ë‚ ì§œ ì¤€ìˆ˜] "2025-08-01" ë‚ ì§œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€? (Boolean)
        2. [ìˆ˜ì¹˜ ì •í™•ì„±] "150ms" ëª©í‘œ ìˆ˜ì¹˜ê°€ ëª…ì‹œë˜ì—ˆëŠ”ê°€? (Boolean)
        3. [í‚¤ì›Œë“œ] "ë§ˆìŠ¤í‚¹" ë˜ëŠ” "Masking" ë‹¨ì–´ê°€ í¬í•¨ë˜ì—ˆëŠ”ê°€? (Boolean)
        4. [í˜•ì‹] Markdown Table í˜•ì‹ì„ ì‚¬ìš©í–ˆëŠ”ê°€? (Boolean)
        5. [ë…¼ë¦¬] "ë¡œê·¸ ëˆ„ë½ë¥  0.1%" ì¡°ê±´ì´ í¬í•¨ë˜ì—ˆëŠ”ê°€? (Boolean)
        """
    else: # Task C
        checklist_prompt = """
        1. [ì¶©ëŒ ë°œê²¬ 1] "ë¬¸ì„œ ë²„ì „" ì¶©ëŒ(2.0 vs 2.1)ì„ ì‹ë³„í–ˆëŠ”ê°€? (Boolean)
        2. [ì¶©ëŒ ë°œê²¬ 2] "ê¸´ê¸‰ ê¶Œí•œ" ì‹œê°„ ì¶©ëŒ(24h vs 4h)ì„ ì‹ë³„í–ˆëŠ”ê°€? (Boolean)
        3. [í˜•ì‹ ì¤€ìˆ˜] "[ì¶©ëŒ N - í•­ëª©ëª…]" í˜•ì‹ì„ ì§€ì¼°ëŠ”ê°€? (Boolean)
        4. [ê·¼ê±° ì œì‹œ] ì¶©ëŒì˜ ê·¼ê±°(ìœ„ì¹˜ ë“±)ë¥¼ ì„¤ëª…í–ˆëŠ”ê°€? (Boolean)
        5. [íŒë‹¨ ë³´ë¥˜] AIê°€ ì„ì˜ë¡œ ê²°ì •í•˜ì§€ ì•Šê³  ë‘ ê°’ì„ ëª¨ë‘ ë³´ê³ í–ˆëŠ”ê°€? (Boolean)
        """

    judge_prompt = f"""
    ë‹¹ì‹ ì€ ëƒ‰ì •í•œ ì±„ì ê´€ì…ë‹ˆë‹¤. ì•„ë˜ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ Pass/Fail ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ì„¸ìš”.
    
    [ì°¸ê°€ì ì‚°ì¶œë¬¼]:
    {user_output[:3000]}
    
    [ì²´í¬ë¦¬ìŠ¤íŠ¸]:
    {checklist_prompt}
    
    JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”:
    {{
        "checks": {{ "check_1": boolean, "check_2": boolean, "check_3": boolean, "check_4": boolean, "check_5": boolean }},
        "feedback": "ê°„ë‹¨í•œ í”¼ë“œë°± (í•œê¸€)"
    }}
    """
    
    resp = await safe_api_call(
        client, model, 
        [{"role": "system", "content": "Output JSON only."}, {"role": "user", "content": judge_prompt}],
        response_format={"type": "json_object"}
    )
    
    try:
        result = json.loads(resp.choices[0].message.content)
        checks = result.get("checks", {})
        true_count = sum(1 for v in checks.values() if v)
        total_score = true_count * 20 # 5ê°œ í•­ëª© * 20ì  = 100ì 
        
        return {
            "score": total_score,
            "feedback": result.get("feedback", ""),
            "details": checks
        }
    except:
        return {"score": 0, "feedback": "ì±„ì  ì‹¤íŒ¨ (JSON íŒŒì‹± ì˜¤ë¥˜)", "details": {}}

# ---------------------------------------------------------
# [í‰ê°€ ë¡œì§ 2] ê³¼ì œ B : Python Code Judge (Data Comparison)
# ---------------------------------------------------------
def evaluate_excel_data(golden_sheets, user_output_text):
    """
    [ìˆ˜ì •ë¨] ë©€í‹° ì‹œíŠ¸ ì§€ì› í‰ê°€ ë¡œì§
    """
    score = 0
    feedback = []
    
    if not golden_sheets:
        return {"score": 0, "feedback": "ì •ë‹µ(Golden) ì—‘ì…€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}

    # 1. ì‚¬ìš©ì ë‹µë³€ì—ì„œ DataFrameë“¤ ì¶”ì¶œ
    user_dfs = extract_csv_dataframes(user_output_text)
    
    if not user_dfs:
        return {"score": 0, "feedback": "í˜•ì‹ ì˜¤ë¥˜: CSV ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë§ˆí¬ë‹¤ìš´ì´ë‚˜ ì‰¼í‘œ êµ¬ë¶„ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”)"}

    # 2. ì •ë‹µ ì‹œíŠ¸ ì¤€ë¹„
    gold_sheet_names = list(golden_sheets.keys())
    gold_dfs = list(golden_sheets.values())
    
    # í‰ê°€ ë£¨í”„
    matched_sheets = 0
    
    # ìµœëŒ€ 2ê°œ ì‹œíŠ¸ê¹Œì§€ë§Œ í‰ê°€ (Sheet1: ë°ì´í„°, Sheet2: ì§‘ê³„)
    max_checks = min(len(gold_dfs), 2)
    
    for i in range(max_checks):
        g_name = gold_sheet_names[i]
        g_df = gold_dfs[i]
        
        # ì‚¬ìš©ìê°€ ìƒì„±í•œ í‘œê°€ ë¶€ì¡±í•˜ë©´ ìŠ¤í‚µ
        if i >= len(user_dfs):
            feedback.append(f"[{g_name}] ëˆ„ë½ë¨ (-50)")
            continue
            
        u_df = user_dfs[i]
        
        # --- ê°œë³„ ì‹œíŠ¸ ì±„ì  ë¡œì§ ---
        sheet_score = 0
        sheet_feedback = []
        
        # 1) ì»¬ëŸ¼ëª… ë¹„êµ (ìœ ì‚¬ë„ ì²´í¬)
        g_cols = set(g_df.columns)
        u_cols = set(u_df.columns)
        common_cols = g_cols.intersection(u_cols)
        
        if len(common_cols) / len(g_cols) >= 0.5: # ì»¬ëŸ¼ì´ 50% ì´ìƒ ì¼ì¹˜í•˜ë©´ ì±„ì  ì§„í–‰
            sheet_score += 20
            
            # 2) í–‰ ê°œìˆ˜ ë¹„êµ
            row_diff = abs(len(g_df) - len(u_df))
            if row_diff == 0:
                sheet_score += 30
                sheet_feedback.append("í–‰ ê°œìˆ˜ ì •í™•")
            elif row_diff < 5: # ì˜¤ì°¨ ë²”ìœ„ í—ˆìš©
                sheet_score += 15
                sheet_feedback.append("í–‰ ê°œìˆ˜ ìœ ì‚¬")
            else:
                sheet_feedback.append(f"í–‰ ê°œìˆ˜ ì°¨ì´ í¼(ì •ë‹µ:{len(g_df)} vs ì œì¶œ:{len(u_df)})")
                
            # 3) ë°ì´í„° ê°’ ì •ë°€ ë¹„êµ (ê°„ì†Œí™”ëœ ë¡œì§)
            # ì²«ë²ˆì§¸ ì»¬ëŸ¼(ë³´í†µ IDë‚˜ Name)ì´ ê°™ì€ì§€ í™•ì¸
            try:
                col_name = list(g_df.columns)[0]
                if col_name in u_df.columns:
                    match_cnt = sum(g_df[col_name].astype(str).str.strip() == u_df[col_name].astype(str).str.strip())
                    accuracy = match_cnt / len(g_df)
                    if accuracy > 0.8: sheet_score += 50
                    elif accuracy > 0.5: sheet_score += 30
                    else: sheet_feedback.append("ë°ì´í„° ê°’ ë¶ˆì¼ì¹˜ ë‹¤ìˆ˜")
            except:
                pass
                
        else:
            sheet_feedback.append("ì»¬ëŸ¼ êµ¬ì¡° ë¶ˆì¼ì¹˜")
            
        # ì‹œíŠ¸ë³„ ì ìˆ˜ í•©ì‚° (ìµœëŒ€ 50ì ì”© ë°°ë¶„)
        final_sheet_score = min(sheet_score, 100) * 0.5 # ì‹œíŠ¸ë‹¹ 50ì  ë§Œì 
        score += final_sheet_score
        feedback.append(f"[{g_name}: {final_sheet_score}ì ] " + ", ".join(sheet_feedback))

    return {"score": round(score), "feedback": " / ".join(feedback)}


# ---------------------------------------------------------
# [ì»¨íŠ¸ë¡¤ëŸ¬] ê°œë³„ ì°¸ê°€ì ì²˜ë¦¬
# ---------------------------------------------------------
async def process_participant(sem, client, row, context, target_file, task_type):
    name = row.iloc[0]
    prompt = row.iloc[1]
    
    async with sem:
        # 1. ì‹¤í–‰ (Execution)
        user_output = await execute_participant_prompt(client, "gpt-4o-mini", context, prompt, task_type)
        
        # 2. í‰ê°€ (Evaluation) - ê³¼ì œ ìœ í˜•ì— ë”°ë¼ ë¶„ê¸°
        if "Task B" in task_type:
            # ê³¼ì œ BëŠ” ì •ë‹µ íŒŒì¼(Excel)ì´ í•„ìš”
            golden_sheets = load_golden_excel(target_file)
            eval_result = evaluate_excel_data(golden_sheets, user_output)
        else:
            # ê³¼ì œ A/CëŠ” í…ìŠ¤íŠ¸ ê¸°ë°˜ LLM í‰ê°€
            target_text = read_file_content(target_file) # ì •ë‹µì§€ í…ìŠ¤íŠ¸
            eval_result = await evaluate_text_logic(client, "gpt-4o-mini", target_text, user_output, task_type)
            
        return {
            "ì´ë¦„": name,
            "ì´ì ": eval_result['score'],
            "í”¼ë“œë°±": eval_result['feedback'],
            "ê²°ê³¼ë¬¼": user_output[:200] + "..." # ìš”ì•½
        }

async def run_grading_pipeline(api_key, context, target_file, df_p, limit, task_type):
    client = AsyncOpenAI(api_key=api_key)
    sem = asyncio.Semaphore(limit)
    tasks = []
    
    status_box = st.empty()
    progress_bar = st.progress(0)
    
    total = len(df_p)
    start_time = time.time()
    
    # íƒœìŠ¤í¬ ìƒì„±
    for idx, row in df_p.iterrows():
        tasks.append(process_participant(sem, client, row, context, target_file, task_type))
    
    results = []
    completed = 0
    
    for f in asyncio.as_completed(tasks):
        res = await f
        results.append(res)
        completed += 1
        
        # UI ì—…ë°ì´íŠ¸
        elapsed = time.time() - start_time
        speed = elapsed / completed
        remaining = (total - completed) * speed
        
        progress_bar.progress(completed / total)
        status_box.markdown(f"""
            <div class='status-box'>
            ğŸš€ {task_type} ì±„ì  ì¤‘... ({completed}/{total})<br>
            ë‚¨ì€ ì‹œê°„: ì•½ {int(remaining)}ì´ˆ
            </div>
        """, unsafe_allow_html=True)
        
    status_box.markdown(f"<div class='status-box success-box'>âœ… ì±„ì  ì™„ë£Œ! ({int(elapsed)}ì´ˆ ì†Œìš”)</div>", unsafe_allow_html=True)
    return pd.DataFrame(results)

# ---------------------------------------------------------
# [ë©”ì¸] UI êµ¬ì„±
# ---------------------------------------------------------
with st.sidebar:
    st.title("ğŸ›ï¸ ì„¤ì • ë° ì—…ë¡œë“œ")
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        api_key = st.text_input("OpenAI API Key", type="password")
        
    st.divider()
    
    # ğŸ¯ ê³¼ì œ ì„ íƒ ê¸°ëŠ¥ ì¶”ê°€
    task_type = st.radio(
        "í‰ê°€í•  ê³¼ì œ ìœ í˜• ì„ íƒ",
        ["Task A (ë¬¸ì„œ êµ¬ì¡°í™”)", "Task B (ë°ì´í„° ì •ì œ)", "Task C (ë…¼ë¦¬ ì¶©ëŒ)"],
        index=0
    )
    
    st.info(f"â„¹ï¸ ì„ íƒëœ ë¡œì§: {'Python Code Judge (Pandas)' if 'Task B' in task_type else 'LLM Judge (Atomic Check)'}")

    st.divider()
    uploaded_ctx = st.file_uploader("1. ë¬¸ë§¥ ìë£Œ (Context)", type=['txt', 'pdf', 'xlsx'])
    uploaded_tgt = st.file_uploader("2. ì •ë‹µ/ê¸°ì¤€ íŒŒì¼ (Golden)", type=['txt', 'xlsx'])
    uploaded_usr = st.file_uploader("3. ì°¸ê°€ì ëª…ë‹¨ (Excel)", type=['xlsx'])

st.title(f"ğŸ† AI í”„ë¡¬í”„íŠ¸ í‰ê°€ ì‹œìŠ¤í…œ : {task_type.split('(')[0]}")

if st.button("ğŸ”¥ ì±„ì  ì‹œì‘", type="primary", use_container_width=True):
    if not (api_key and uploaded_ctx and uploaded_tgt and uploaded_usr):
        st.warning("âš ï¸ API Keyì™€ ëª¨ë“  íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    else:
        # íŒŒì¼ ë‚´ìš© ì½ê¸° (ê³¼ì œ Bì˜ Targetì€ ì—¬ê¸°ì„œ ì½ì§€ ì•Šê³  í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬)
        context_text = read_file_content(uploaded_ctx)
        df_participants = pd.read_excel(uploaded_usr)
        
        # ë¹„ë™ê¸° ì‹¤í–‰
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result_df = loop.run_until_complete(
                run_grading_pipeline(api_key, context_text, uploaded_tgt, df_participants, 10, task_type)
            )
            
            # ê²°ê³¼ í‘œì‹œ
            st.divider()
            col1, col2 = st.columns([1, 3])
            
            # ìƒìœ„ 3ëª…
            top_rank = result_df.sort_values(by="ì´ì ", ascending=False).head(3)
            col1.subheader("ğŸ¥‡ Top 3")
            col1.table(top_rank[["ì´ë¦„", "ì´ì "]])
            
            # ì „ì²´ í…Œì´ë¸”
            col2.subheader("ğŸ“‹ ì „ì²´ ê²°ê³¼")
            st.dataframe(
                result_df.sort_values(by="ì´ì ", ascending=False),
                use_container_width=True,
                column_config={
                    "ì´ì ": st.column_config.ProgressColumn("Score", format="%dì ", min_value=0, max_value=100),
                }
            )
            
            # ë‹¤ìš´ë¡œë“œ
            output = io.BytesIO()
            result_df.to_excel(output, index=False)
            st.download_button("ğŸ“¥ ê²°ê³¼ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", output.getvalue(), "evaluation_result.xlsx")
            
        except Exception as e:
            st.error(f"ì‹œìŠ¤í…œ ì—ëŸ¬: {str(e)}")
